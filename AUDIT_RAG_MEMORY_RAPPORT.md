# AUDIT COMPLET : SYST√àMES RAG & MEMORY

**Date:** 21 Novembre 2025
**Auditeur:** Claude Agent
**Scope:** Organisation, stockage, flux et r√®gles des syst√®mes RAG et Memory
**Objectif:** Comprendre l'√©tat actuel et proposer des am√©liorations architecturales

---

## TABLE DES MATI√àRES

1. [Vue d'ensemble](#vue-densemble)
2. [Syst√®me RAG - Architecture Actuelle](#syst√®me-rag---architecture-actuelle)
3. [Syst√®me Memory - Architecture Actuelle](#syst√®me-memory---architecture-actuelle)
4. [Flux de Donn√©es](#flux-de-donn√©es)
5. [R√®gles de Tri et Rangement](#r√®gles-de-tri-et-rangement)
6. [Probl√®mes Identifi√©s](#probl√®mes-identifi√©s)
7. [Propositions d'Am√©lioration](#propositions-dam√©lioration)
8. [Plan d'Action Recommand√©](#plan-daction-recommand√©)

---

## VUE D'ENSEMBLE

### Architecture G√©n√©rale

Le syst√®me dispose de **deux syst√®mes de m√©moire distincts** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     AGENT ORCHESTRATOR                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ  RAG SYSTEM  ‚îÇ              ‚îÇ MEMORY SYSTEM  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  (Long-term) ‚îÇ              ‚îÇ  (Short-term)  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ              ‚îÇ                ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  Documents   ‚îÇ              ‚îÇ  Conversations ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  Embeddings  ‚îÇ              ‚îÇ  Sessions      ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  Semantic    ‚îÇ              ‚îÇ  Text Search   ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ         ‚Üì                              ‚Üì                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ  SQLite DB   ‚îÇ              ‚îÇ   JSON Files   ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  rag.db      ‚îÇ              ‚îÇ  memory_data/  ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ         CONTEXT BUILDER                      ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  (Agr√®ge RAG + Memory + Vision + System)    ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ              TIMELINE                        ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  (Historique chronologique des actions)     ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants Analys√©s

**Fichiers Audit√©s:**
- `backend/rag/rag_store.py` - Stockage RAG avec embeddings
- `backend/connectors/memory/memory_manager.py` - Gestionnaire m√©moire persistante
- `backend/rag/rag_engine.py` - Sessions conversation courte dur√©e
- `backend/orchestrator/context_builder.py` - Agr√©gation contextes
- `backend/orchestrator/timeline.py` - Historique √©v√©nements
- `backend/orchestrator/clients/rag_client.py` - Client RAG MCP
- `backend/orchestrator/clients/memory_client.py` - Client Memory MCP
- `backend/routes/rag_routes.py` - API RAG
- `backend/routes/memory_route.py` - API Memory

---

## SYST√àME RAG - ARCHITECTURE ACTUELLE

### 1. Structure de Stockage

#### Base de Donn√©es SQLite

**Localisation:** `C:\AGENT LOCAL\rag\rag.db`
**Taille:** 60 KB
**Technologie:** SQLite + Sentence-Transformers (all-MiniLM-L6-v2)

#### Sch√©ma de Tables

```sql
-- Table: documents
CREATE TABLE documents (
    id TEXT PRIMARY KEY,              -- SHA256 hash
    dataset TEXT NOT NULL,            -- Nom du dataset
    filename TEXT NOT NULL,           -- Nom du fichier
    content TEXT NOT NULL,            -- Contenu complet
    metadata TEXT,                    -- JSON metadata
    created_at TEXT NOT NULL,         -- ISO timestamp
    updated_at TEXT NOT NULL          -- ISO timestamp
);

-- Table: chunks
CREATE TABLE chunks (
    id TEXT PRIMARY KEY,              -- document_id_index
    document_id TEXT NOT NULL,        -- FK vers documents
    chunk TEXT NOT NULL,              -- Texte du chunk
    embedding TEXT,                   -- JSON array embeddings (384 dims)
    order_index INTEGER NOT NULL,    -- Position dans le document
    created_at TEXT NOT NULL,         -- ISO timestamp
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
);

-- Indexes
CREATE INDEX idx_documents_dataset ON documents(dataset);
CREATE INDEX idx_chunks_document ON chunks(document_id);
```

### 2. Datasets D√©finis

Le syst√®me d√©finit **5 datasets conceptuels** :

```python
VALID_DATASETS = {
    "agent_core",    # R√®gles permanentes, identit√©, structure PC
    "context_flow",  # R√©sum√©s pr√©/post, flux conversationnel
    "agent_memory",  # Feedbacks, le√ßons apprises
    "projects",      # Code, documentation analytique
    "scratchpad"     # Temporaire (legacy)
}
```

#### Mapping d'Alias

```python
mapping = {
    "core": "agent_core",
    "rules": "agent_core",
    "memory": "agent_memory",
    "feedback": "agent_memory",
    "lessons": "agent_memory",
    "context": "context_flow",
    "summary": "context_flow",
    "flow": "context_flow",
    "project": "projects",
    "code": "projects",
    "docs": "projects",
    "temp": "scratchpad"
}
```

### 3. √âtat Actuel de la Base

**Donn√©es pr√©sentes:**
- **Datasets:** `test_dataset` (2 docs), `uploads` (1 doc)
- **Total chunks:** 3
- **Total documents:** 3

‚ö†Ô∏è **OBSERVATION CRITIQUE:** Les datasets conceptuels d√©finis dans le code (`agent_core`, `context_flow`, etc.) **ne sont pas utilis√©s** dans la base actuelle. Les donn√©es sont stock√©es dans des datasets g√©n√©riques.

### 4. Traitement des Documents

#### Chunking
```python
def _chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200)
```
- **Taille chunk:** 1000 caract√®res
- **Overlap:** 200 caract√®res
- **M√©thode:** D√©coupage simple sans respect des limites s√©mantiques

#### Embeddings
- **Mod√®le:** `all-MiniLM-L6-v2` (Sentence-Transformers)
- **Dimensions:** 384
- **Type:** Local (pas d'API externe)
- **Stockage:** JSON string dans SQLite

#### Recherche S√©mantique
```python
def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float
```
- **M√©thode:** Cosine similarity
- **Algorithme:** NumPy dot product + norms
- **Tri:** Par score de similarit√© d√©croissant

### 5. Flux d'Ajout de Documents

```
User Request
    ‚Üì
add_document(dataset, filename, content, metadata)
    ‚Üì
1. G√©n√®re document_id (SHA256 hash)
2. Stocke document dans table documents
3. Chunke le contenu (1000 chars, overlap 200)
4. Pour chaque chunk:
    - G√©n√®re embedding (384 dims)
    - Stocke chunk + embedding dans table chunks
    ‚Üì
Retourne document_id
```

### 6. Flux de Requ√™te

```
User Query
    ‚Üì
query(dataset, question, top_k=5)
    ‚Üì
1. G√©n√®re embedding de la question
2. R√©cup√®re tous les chunks du dataset
3. Calcule cosine similarity pour chaque chunk
4. Trie par similarit√© d√©croissante
5. Retourne top_k chunks
    ‚Üì
Retourne [{chunk_id, content, filename, metadata, similarity}]
```

### 7. Op√©rations Support√©es

| Op√©ration | Description | Endpoint |
|-----------|-------------|----------|
| `add_document` | Ajouter un document | POST /rag/add_document |
| `query` | Recherche s√©mantique | POST /rag/query |
| `list_documents` | Lister tous les docs | GET /rag/list_documents |
| `list_datasets` | Lister les datasets | GET /rag/list_datasets |
| `get_dataset_info` | Info sur un dataset | GET /rag/get_dataset_info |
| `delete_document` | Supprimer un doc | DELETE /rag/delete_document |
| `delete_dataset` | Supprimer un dataset | DELETE /rag/delete_dataset |
| `get_document_chunks` | Chunks d'un doc | GET /rag/get_document_chunks |
| `cleanup_memory` | Nettoyage scratchpad | POST /rag/cleanup_memory |

### 8. M√©tadonn√©es

```python
def parse_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    # Types valides
    valid_types = {"core_rule", "context_data", "learning_data", "project_doc", "general"}

    # Priorit√©s valides
    valid_priorities = {"high", "medium", "low"}
```

**Structure metadata:**
```json
{
  "type": "core_rule|context_data|learning_data|project_doc|general",
  "priority": "high|medium|low",
  "dataset": "agent_core|context_flow|...",
  "filename": "nom_fichier.txt",
  "...": "champs personnalis√©s"
}
```

---

## SYST√àME MEMORY - ARCHITECTURE ACTUELLE

### 1. Structure de Stockage

#### Fichiers JSON

**Localisation:** `C:\AGENT LOCAL\memory_data/`
**Format:** Un fichier JSON par session
**Naming:** `{session_id}.json`

**Nombre actuel:** 33 sessions enregistr√©es

#### Structure d'un Fichier Session

```json
{
  "session_id": "default",
  "created_at": "2025-11-21T03:09:00.123456",
  "updated_at": "2025-11-21T05:21:00.789012",
  "messages": [
    {
      "role": "user|assistant|system",
      "content": "Contenu du message",
      "timestamp": "2025-11-21T03:09:01.234567",
      "metadata": {
        // Champs optionnels
      }
    }
  ]
}
```

### 2. Double Syst√®me de Sessions

‚ö†Ô∏è **DUPLICATION CRITIQUE D√âTECT√âE**

Il existe **DEUX syst√®mes de gestion de sessions** :

#### A. MemoryManager (`backend/connectors/memory/memory_manager.py`)
- **Stockage:** `C:\AGENT LOCAL\memory_data/`
- **Format:** JSON (un fichier par session)
- **Utilisation:** API Memory principale
- **Endpoints:** `/memory/*`

#### B. SessionMemory (`backend/rag/rag_engine.py`)
- **Stockage:** `C:\AGENT LOCAL\memory/sessions/`
- **Format:** JSONL (append-only log)
- **Utilisation:** Sessions RAG court terme
- **√âtat:** R√©pertoire **vide** (non utilis√©)

### 3. Flux d'Ajout de Message

```
User Message
    ‚Üì
add_message(session_id, role, content, metadata)
    ‚Üì
1. Charge ou cr√©e le fichier {session_id}.json
2. Ajoute le message avec timestamp
3. Met √† jour updated_at
4. Sauvegarde le fichier
    ‚Üì
Retourne success
```

### 4. Op√©rations Support√©es

| Op√©ration | Description | Endpoint |
|-----------|-------------|----------|
| `add` | Ajouter un message | POST /memory/add |
| `get` | R√©cup√©rer messages | POST /memory/get |
| `get_context` | Contexte format√© | GET /memory/session/{id}/context |
| `search` | Recherche textuelle | POST /memory/search |
| `clear` | Effacer session | POST /memory/clear |
| `list_sessions` | Lister sessions | GET /memory/sessions |
| `get_summary` | R√©sum√© session | GET /memory/session/{id}/summary |
| `get_full_session` | Session compl√®te | GET /memory/session/{id} |

### 5. Recherche

**Type:** Recherche textuelle simple (substring matching)
**M√©thode:** `query.lower() in content.lower()`
**Scope:** Une session OU toutes les sessions

‚ö†Ô∏è **LIMITATION:** Pas d'embeddings, pas de recherche s√©mantique dans Memory

### 6. Contexte Format√©

```python
def get_context(session_id: str, max_messages: int = 10) -> str:
    # Format:
    # Previous conversation:
    # [user] Message user
    # [assistant] R√©ponse assistant
    # ...
```

---

## FLUX DE DONN√âES

### 1. Architecture des Flux

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    USER REQUEST                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚Üì
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  FastAPI Main (main.py)    ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ
        ‚Üì                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAG Routes   ‚îÇ   ‚îÇ  Memory Routes   ‚îÇ
‚îÇ  /rag/*       ‚îÇ   ‚îÇ  /memory/*       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                    ‚îÇ
        ‚Üì                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAGStore     ‚îÇ   ‚îÇ  MemoryManager   ‚îÇ
‚îÇ  (rag.db)     ‚îÇ   ‚îÇ  (JSON files)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Orchestrator  ‚îÇ
        ‚îÇ  + Clients     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Context Builder‚îÇ
        ‚îÇ  (Agr√©gation)  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Sources de Donn√©es

#### A. RAG (Documents Long-Terme)

**Alimentation:**
1. **Manuellement** via API `/rag/add_document`
2. **Par l'orchestrateur** via `rag_client.add_document()`
3. **Non automatique** - requiert action explicite

**Contenu attendu:**
- R√®gles de l'agent
- Documentation projet
- Code analys√©
- Connaissances persistantes
- Le√ßons apprises

#### B. Memory (Conversations Court-Terme)

**Alimentation:**
1. **Automatique** - chaque message user/assistant
2. **Via orchestrateur** - `memory_client.add_message()`
3. **Temps r√©el** - append √† chaque interaction

**Contenu attendu:**
- Historique conversationnel
- Messages user/assistant/system
- Contexte de session
- √âtat dialogue

#### C. Timeline (√âv√©nements)

**Alimentation:**
1. **Automatique** - chaque action de l'orchestrateur
2. **En m√©moire** - pas de persistance
3. **Limit√©** - max 1000 √©v√©nements

**Contenu:**
- Plans g√©n√©r√©s
- Ex√©cutions d'outils
- Erreurs
- √âv√©nements multimodaux (audio, vision)

#### D. Context Builder (Agr√©gation)

**Sources consolid√©es:**
```python
async def build_super_context(user_message, session_id):
    return {
        "memory": await _get_memory_context(),      # Memory + search
        "rag_docs": await _get_rag_context(),       # RAG multi-datasets
        "vision": await _get_vision_context(),       # Vision active
        "system_state": await _get_system_state(),   # Snapshot syst√®me
        "audio": await _get_audio_context(),         # Audio r√©cent
        "documents": await _get_documents_context()  # Docs r√©cents
    }
```

### 3. Flux de Requ√™te Utilisateur

```
1. User message arrive
    ‚Üì
2. Orchestrator re√ßoit la requ√™te
    ‚Üì
3. Context Builder agr√®ge:
    - Memory: get_context(session_id, max=5) + search(user_message)
    - RAG: query multiple datasets
        * agent_core (top_k=2)
        * projects (top_k=2)
        * scratchpad (top_k=1)
        * rules (top_k=1)
    - Vision: contexte actif
    - System: snapshot √©tat
    - Audio: contexte r√©cent
    - Documents: documents r√©cents
    ‚Üì
4. Super-context construit
    ‚Üì
5. LLM Planner g√©n√®re plan avec contexte
    ‚Üì
6. Executor ex√©cute actions
    ‚Üì
7. R√©ponse finale user
    ‚Üì
8. Memory stocke interaction (add_message)
9. Timeline enregistre √©v√©nements
```

---

## R√àGLES DE TRI ET RANGEMENT

### 1. RAG - R√®gles de Datasets

#### Mapping Conceptuel

```python
# D√©finition th√©orique (EnhancedRAGStore)
VALID_DATASETS = {
    "agent_core",    # Permanent: Identity, Rules, PC Structure
    "context_flow",  # Pre/Post summaries, conversational flow
    "agent_memory",  # Feedbacks, lessons learned
    "projects",      # Code, analytical docs, medium-term work
    "scratchpad"     # Ephemeral, temporary info
}
```

#### Auto-Routing

La classe `EnhancedRAGStore` impl√©mente un **auto-routing** :

```python
async def auto_route(self, dataset_choice: str) -> str:
    # Alias ‚Üí dataset canonique
    if dataset_choice in VALID_DATASETS:
        return dataset_choice
    return mapping.get(dataset_choice.lower(), "scratchpad")
```

‚ö†Ô∏è **PROBL√àME:** Cette logique existe mais **n'est PAS appliqu√©e** syst√©matiquement.

#### R√®gles de Priorit√©

```python
valid_types = {"core_rule", "context_data", "learning_data", "project_doc", "general"}
valid_priorities = {"high", "medium", "low"}
```

**Mapping type ‚Üí dataset (logique attendue):**
- `core_rule` ‚Üí `agent_core`
- `context_data` ‚Üí `context_flow`
- `learning_data` ‚Üí `agent_memory`
- `project_doc` ‚Üí `projects`
- `general` ‚Üí `scratchpad`

‚ö†Ô∏è **PROBL√àME:** Cette logique n'est **pas impl√©ment√©e** dans `parse_metadata()`.

### 2. Memory - R√®gles de Sessions

#### Naming Convention

```python
# Sanitized session_id
safe_session_id = "".join(c for c in session_id if c.isalnum() or c in ('-', '_'))
filename = f"{safe_session_id}.json"
```

**Pattern observ√©:**
- `default.json` - Session par d√©faut
- `session_{timestamp}_{random}.json` - Sessions g√©n√©r√©es
- `test_*.json` - Sessions de test
- `{custom_name}.json` - Sessions nomm√©es

#### Pas de Tri Hi√©rarchique

‚ö†Ô∏è **OBSERVATION:** Tous les fichiers sont au m√™me niveau dans `memory_data/`. Aucune organisation par :
- Date
- Utilisateur
- Projet
- Type d'interaction

#### Pas de R√©tention

‚ö†Ô∏è **OBSERVATION:** Aucune r√®gle de r√©tention automatique. Les sessions s'accumulent ind√©finiment.

### 3. Timeline - R√®gles d'√âv√©nements

#### D√©tection Automatique de Modalit√©

```python
def _detect_modality(event_type: str, data: Dict[str, Any]) -> str:
    # R√®gles de d√©tection:
    if "audio" in event_type.lower():
        return "audio"
    elif "vision" in event_type.lower() or "image" in event_type.lower():
        return "vision"
    elif "document" in event_type.lower():
        return "documents"
    elif "system" in event_type.lower():
        return "system"
    # V√©rifier tool dans data
    return "text"  # default
```

#### Limitation de Taille

```python
self.max_events = 1000  # Limite m√©moire
if len(self.events) > self.max_events:
    self.events = self.events[-self.max_events:]  # Garde les plus r√©cents
```

‚ö†Ô∏è **PROBL√àME:** Pas de persistance. En cas de red√©marrage, toute la timeline est perdue.

### 4. Context Builder - R√®gles d'Agr√©gation

#### Requ√™tes RAG Multi-Datasets

```python
# A. CORE MEMORY (top_k=2)
core_results = await rag_client.query("agent_core", user_message, top_k=2)

# B. PROJECT MEMORY (top_k=2)
project_results = await rag_client.query("projects", user_message, top_k=2)

# C. SCRATCHPAD (top_k=1)
scratch_results = await rag_client.query("scratchpad", user_message, top_k=1)

# D. RULES (top_k=1)
rules_results = await rag_client.query("rules", user_message, top_k=1)
```

**Total chunks r√©cup√©r√©s:** Max 6 chunks par requ√™te

#### Requ√™te Memory

```python
# Contexte r√©cent: 5 derniers messages
context = await memory_client.get_context(session_id, max_messages=5)

# Recherche s√©mantique (textuelle)
search_results = await memory_client.search(user_message, session_id)
```

---

## PROBL√àMES IDENTIFI√âS

### üî¥ CRITIQUES

#### 1. Datasets RAG Non Utilis√©s

**Probl√®me:** Le code d√©finit 5 datasets conceptuels (`agent_core`, `context_flow`, `agent_memory`, `projects`, `scratchpad`) mais la base de donn√©es contient des datasets g√©n√©riques (`test_dataset`, `uploads`).

**Impact:**
- ‚ùå Perte de la structure s√©mantique
- ‚ùå Pas de s√©paration m√©moire permanente vs temporaire
- ‚ùå Confusion sur o√π ranger les documents
- ‚ùå Auto-routing non effectif

**Cause:** Absence d'utilisation coh√©rente de `EnhancedRAGStore` et son `auto_route()`.

#### 2. Double Syst√®me de Sessions

**Probl√®me:** Deux syst√®mes de gestion de sessions coexistent :
- `MemoryManager` (JSON, utilis√©)
- `SessionMemory` (JSONL, non utilis√©)

**Impact:**
- ‚ùå Confusion architecturale
- ‚ùå Code mort (`rag_engine.py` sessions)
- ‚ùå Risque d'utilisation incoh√©rente
- ‚ùå Maintenance compliqu√©e

#### 3. Memory Sans Recherche S√©mantique

**Probl√®me:** La recherche dans Memory est purement textuelle (substring matching), contrairement au RAG qui a des embeddings.

**Impact:**
- ‚ùå Impossible de retrouver des conversations par sens
- ‚ùå Recherche tr√®s limit√©e
- ‚ùå Pas d'exploitation de la s√©mantique des √©changes

**Exemple:** Rechercher "comment configurer l'audio" ne trouvera pas "setup microphone" ou "param√©trer le son".

#### 4. Timeline Non Persistante

**Probl√®me:** La Timeline est en m√©moire (liste Python), limit√©e √† 1000 √©v√©nements, et perdue au red√©marrage.

**Impact:**
- ‚ùå Perte de l'historique des actions au red√©marrage
- ‚ùå Impossible d'auditer les actions pass√©es
- ‚ùå Limite de 1000 √©v√©nements arbitraire
- ‚ùå Pas de tra√ßabilit√© long terme

### üü† MAJEURS

#### 5. M√©tadonn√©es Non Exploit√©es

**Probl√®me:** Le syst√®me d√©finit `parse_metadata()` avec types et priorit√©s, mais ne les utilise pas pour :
- Router automatiquement vers le bon dataset
- Prioriser les r√©sultats de recherche
- Organiser les documents

**Impact:**
- ‚ö†Ô∏è M√©tadonn√©es inutilis√©es
- ‚ö†Ô∏è Pas de filtrage par type
- ‚ö†Ô∏è Pas de tri par priorit√©

#### 6. Chunking Na√Øf

**Probl√®me:** Le d√©coupage est fixe (1000 chars, overlap 200), sans respect des:
- Paragraphes
- Phrases
- Sections logiques

**Impact:**
- ‚ö†Ô∏è Chunks coup√©s au milieu de phrases
- ‚ö†Ô∏è Perte de contexte s√©mantique
- ‚ö†Ô∏è Qualit√© de recherche d√©grad√©e

#### 7. Pas de Cleanup Automatique

**Probl√®me:** Aucun syst√®me de r√©tention automatique pour :
- `scratchpad` dans RAG
- Sessions anciennes dans Memory
- Timeline (mais en m√©moire donc reset au reboot)

**Impact:**
- ‚ö†Ô∏è Accumulation de donn√©es obsol√®tes
- ‚ö†Ô∏è Croissance continue de la base
- ‚ö†Ô∏è Pollution des r√©sultats de recherche

#### 8. Absence de Versioning

**Probl√®me:** Les documents dans RAG peuvent √™tre mis √† jour (INSERT OR REPLACE) mais sans historique de versions.

**Impact:**
- ‚ö†Ô∏è Impossible de revenir en arri√®re
- ‚ö†Ô∏è Perte de l'√©volution des documents
- ‚ö†Ô∏è Pas d'audit des modifications

### üü° MINEURS

#### 9. Context Builder Trop G√©n√©rique

**Probl√®me:** Le Context Builder r√©cup√®re toujours les m√™mes datasets RAG, sans adaptation au type de requ√™te.

**Impact:**
- ‚öôÔ∏è Surcharge inutile pour requ√™tes simples
- ‚öôÔ∏è Pas d'optimisation par contexte

#### 10. Embeddings en JSON

**Probl√®me:** Les embeddings (384 dimensions) sont stock√©s en JSON text dans SQLite, pas en BLOB binaire.

**Impact:**
- ‚öôÔ∏è Taille de stockage 4-5x plus grande
- ‚öôÔ∏è Parsing JSON √† chaque requ√™te
- ‚öôÔ∏è Performance d√©grad√©e

#### 11. Recherche Non Optimis√©e

**Probl√®me:** Pour chaque requ√™te, TOUS les chunks du dataset sont charg√©s en m√©moire pour calculer les similarit√©s.

**Impact:**
- ‚öôÔ∏è Scalabilit√© limit√©e
- ‚öôÔ∏è Performance O(n) o√π n = nombre de chunks
- ‚öôÔ∏è Pas d'indexation vectorielle

#### 12. Sessions Memory Non Structur√©es

**Probl√®me:** Toutes les sessions sont au m√™me niveau dans `memory_data/`, sans organisation hi√©rarchique.

**Impact:**
- ‚öôÔ∏è Difficile de naviguer avec 33+ sessions
- ‚öôÔ∏è Pas de groupement par projet/utilisateur/date
- ‚öôÔ∏è Backup/archivage compliqu√©

---

## PROPOSITIONS D'AM√âLIORATION

### üöÄ PRIORIT√â 1 - CRITIQUE (Court Terme)

#### A1. Impl√©menter Datasets RAG Correctement

**Objectif:** Utiliser syst√©matiquement les 5 datasets conceptuels

**Actions:**
1. Migrer les donn√©es existantes vers les bons datasets
2. Forcer l'utilisation de `EnhancedRAGStore` partout
3. Documenter le mapping dataset ‚Üí usage
4. Ajouter validation √† l'API

**Impl√©mentation:**
```python
# Dans routes/rag_routes.py - ajouter validation
ALLOWED_DATASETS = {"agent_core", "context_flow", "agent_memory", "projects", "scratchpad"}

@router.post("/rag/add_document")
async def add_document(request: AddDocumentRequest):
    dataset = request.dataset
    if dataset not in ALLOWED_DATASETS:
        raise HTTPException(400, f"Invalid dataset. Use: {ALLOWED_DATASETS}")
    # Auto-route si alias fourni
    dataset = await rag_store.auto_route(dataset)
    ...
```

**Script de migration:**
```python
# migration_datasets.py
import sqlite3

conn = sqlite3.connect('rag/rag.db')
cursor = conn.cursor()

# Mapper test_dataset ‚Üí projects (exemple)
cursor.execute("UPDATE documents SET dataset='projects' WHERE dataset='test_dataset'")
cursor.execute("UPDATE documents SET dataset='scratchpad' WHERE dataset='uploads'")

conn.commit()
conn.close()
```

#### A2. Supprimer le Double Syst√®me de Sessions

**Objectif:** Garder uniquement `MemoryManager`, supprimer `SessionMemory`

**Actions:**
1. Supprimer les fonctions dans `rag_engine.py`
2. Supprimer le r√©pertoire `memory/sessions/`
3. Mettre √† jour les imports
4. Clarifier la documentation

**Fichiers √† modifier:**
- `backend/rag/rag_engine.py` - Supprimer `add_message_to_session`, `get_session_history`
- `backend/memory/sessions/` - Supprimer r√©pertoire

#### A3. Ajouter Embeddings √† Memory

**Objectif:** Permettre recherche s√©mantique dans les conversations

**Impl√©mentation:**
```python
# Dans MemoryManager
class MemoryManager:
    def __init__(self, storage_dir: Optional[str] = None):
        # ...
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def add(self, session_id, message, role, metadata):
        # G√©n√©rer embedding du message
        embedding = self.embedding_model.encode(message).tolist()

        message_entry = {
            "role": role,
            "content": message,
            "embedding": embedding,  # NOUVEAU
            "timestamp": datetime.now().isoformat()
        }
        # ...

    async def semantic_search(self, query: str, session_id: Optional[str] = None, top_k: int = 5):
        """Recherche s√©mantique dans les conversations"""
        query_embedding = self.embedding_model.encode(query).tolist()

        # Charger toutes les sessions concern√©es
        # Calculer similarit√©s
        # Retourner top_k
```

#### A4. Persister la Timeline

**Objectif:** Sauvegarder la timeline en base de donn√©es

**Structure:**
```sql
CREATE TABLE timeline_events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,
    session_id TEXT NOT NULL,
    event_type TEXT NOT NULL,
    data TEXT NOT NULL,  -- JSON
    metadata TEXT,        -- JSON
    modality TEXT
);

CREATE INDEX idx_timeline_session ON timeline_events(session_id);
CREATE INDEX idx_timeline_type ON timeline_events(event_type);
CREATE INDEX idx_timeline_modality ON timeline_events(modality);
```

**Impl√©mentation:**
```python
class PersistentTimeline(Timeline):
    def __init__(self, db_path="timeline.db"):
        super().__init__()
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""CREATE TABLE IF NOT EXISTS ...""")
        conn.commit()
        conn.close()

    def add_event(self, event_type, data, session_id, metadata):
        # Ajouter √† self.events (m√©moire)
        event = super().add_event(event_type, data, session_id, metadata)

        # Persister en DB
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("INSERT INTO timeline_events ...")
        conn.commit()
        conn.close()

        return event
```

### üîß PRIORIT√â 2 - MAJEURE (Moyen Terme)

#### B1. M√©tadonn√©es Intelligentes

**Objectif:** Utiliser les m√©tadonn√©es pour router et filtrer

**Impl√©mentation:**
```python
class SmartRAGStore(EnhancedRAGStore):
    async def add_document(self, dataset, filename, content, metadata):
        # Auto-d√©terminer le dataset bas√© sur metadata.type
        if metadata and "type" in metadata:
            doc_type = metadata["type"]
            type_to_dataset = {
                "core_rule": "agent_core",
                "context_data": "context_flow",
                "learning_data": "agent_memory",
                "project_doc": "projects",
                "general": "scratchpad"
            }
            dataset = type_to_dataset.get(doc_type, dataset)

        # Valider et parser metadata
        enhanced_metadata = self.parse_metadata(metadata)

        # Ajouter dataset auto-d√©termin√©
        enhanced_metadata["auto_dataset"] = dataset

        return await super().add_document(dataset, filename, content, enhanced_metadata)

    async def query(self, dataset, question, top_k=5, filters: Dict[str, Any] = None):
        """Query avec filtres metadata"""
        results = await super().query(dataset, question, top_k)

        if filters:
            # Filtrer par type
            if "type" in filters:
                results = [r for r in results if r["metadata"].get("type") == filters["type"]]

            # Filtrer par priorit√©
            if "min_priority" in filters:
                priority_order = {"high": 3, "medium": 2, "low": 1}
                min_val = priority_order.get(filters["min_priority"], 0)
                results = [r for r in results
                          if priority_order.get(r["metadata"].get("priority", "low"), 1) >= min_val]

        return results
```

#### B2. Chunking Intelligent

**Objectif:** Respecter les limites s√©mantiques

**Impl√©mentation:**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

class SmartChunker:
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n",  # Paragraphes
                "\n",    # Lignes
                ". ",    # Phrases
                " ",     # Mots
                ""       # Caract√®res
            ]
        )

    def chunk_text(self, text: str) -> List[str]:
        return self.splitter.split_text(text)
```

#### B3. Cleanup Automatique

**Objectif:** Nettoyer automatiquement les donn√©es obsol√®tes

**Impl√©mentation:**
```python
class RAGStoreWithCleanup(RAGStore):
    async def cleanup_scratchpad(self, retention_days: int = 7):
        """Supprimer les docs du scratchpad plus vieux que N jours"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cutoff_date = (datetime.utcnow() - timedelta(days=retention_days)).isoformat()

        cursor.execute("""
            SELECT id FROM documents
            WHERE dataset = 'scratchpad'
            AND created_at < ?
        """, (cutoff_date,))

        doc_ids = [row[0] for row in cursor.fetchall()]

        for doc_id in doc_ids:
            cursor.execute("DELETE FROM chunks WHERE document_id = ?", (doc_id,))
            cursor.execute("DELETE FROM documents WHERE id = ?", (doc_id,))

        conn.commit()
        conn.close()

        return {"deleted": len(doc_ids), "retention_days": retention_days}

class MemoryManagerWithCleanup(MemoryManager):
    async def cleanup_old_sessions(self, retention_days: int = 30):
        """Archiver sessions plus vieilles que N jours"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        archived_count = 0
        for session_file in self.storage_dir.glob("*.json"):
            if session_file.stat().st_mtime < cutoff_date.timestamp():
                # Archiver dans un sous-r√©pertoire
                archive_dir = self.storage_dir / "archive"
                archive_dir.mkdir(exist_ok=True)
                session_file.rename(archive_dir / session_file.name)
                archived_count += 1

        return {"archived": archived_count, "retention_days": retention_days}
```

**T√¢che cron:**
```python
# Dans orchestrator ou main.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler

scheduler = AsyncIOScheduler()

@scheduler.scheduled_job('cron', hour=2, minute=0)  # 2h du matin chaque jour
async def daily_cleanup():
    # Cleanup scratchpad (7 jours)
    await rag_store.cleanup_scratchpad(retention_days=7)

    # Cleanup sessions (30 jours)
    await memory_manager.cleanup_old_sessions(retention_days=30)

    print(f"[CLEANUP] Daily cleanup completed at {datetime.now()}")

scheduler.start()
```

#### B4. Versioning Documents

**Objectif:** Historiser les modifications de documents

**Structure:**
```sql
CREATE TABLE document_versions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id TEXT NOT NULL,
    version INTEGER NOT NULL,
    content TEXT NOT NULL,
    metadata TEXT,
    created_at TEXT NOT NULL,
    created_by TEXT,
    change_reason TEXT,
    UNIQUE(document_id, version)
);

CREATE INDEX idx_versions_doc ON document_versions(document_id);
```

**Impl√©mentation:**
```python
class VersionedRAGStore(RAGStore):
    async def add_document(self, dataset, filename, content, metadata):
        # V√©rifier si le document existe d√©j√†
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        doc_id = hashlib.sha256(f"{dataset}:{filename}:{content[:100]}".encode()).hexdigest()

        cursor.execute("SELECT COUNT(*) FROM documents WHERE id = ?", (doc_id,))
        exists = cursor.fetchone()[0] > 0

        if exists:
            # Archiver la version actuelle
            cursor.execute("SELECT content, metadata FROM documents WHERE id = ?", (doc_id,))
            old_content, old_metadata = cursor.fetchone()

            # Compter les versions existantes
            cursor.execute("SELECT COALESCE(MAX(version), 0) FROM document_versions WHERE document_id = ?", (doc_id,))
            last_version = cursor.fetchone()[0]
            new_version = last_version + 1

            # Sauvegarder l'ancienne version
            cursor.execute("""
                INSERT INTO document_versions (document_id, version, content, metadata, created_at)
                VALUES (?, ?, ?, ?, ?)
            """, (doc_id, new_version, old_content, old_metadata, datetime.utcnow().isoformat()))

            conn.commit()

        conn.close()

        # Ajouter/mettre √† jour le document
        return await super().add_document(dataset, filename, content, metadata)

    def get_document_history(self, doc_id: str) -> List[Dict[str, Any]]:
        """R√©cup√©rer l'historique des versions d'un document"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT version, content, metadata, created_at
            FROM document_versions
            WHERE document_id = ?
            ORDER BY version DESC
        """, (doc_id,))

        versions = []
        for row in cursor.fetchall():
            versions.append({
                "version": row[0],
                "content": row[1],
                "metadata": json.loads(row[2]) if row[2] else {},
                "created_at": row[3]
            })

        conn.close()
        return versions
```

### ‚ö° PRIORIT√â 3 - OPTIMISATION (Long Terme)

#### C1. Context Builder Adaptatif

**Objectif:** Adapter les requ√™tes RAG selon le type de demande

**Impl√©mentation:**
```python
class AdaptiveContextBuilder(ContextBuilder):
    async def build_super_context(self, user_message: str, session_id: str = "default"):
        # Classifier l'intent du message
        intent = await self._classify_intent(user_message)

        # Adapter les datasets RAG selon l'intent
        rag_config = self._get_rag_config_for_intent(intent)

        # Construire le contexte adapt√©
        memory = await self._get_memory_context(user_message, session_id)
        rag = await self._get_adaptive_rag_context(user_message, rag_config)

        # Contextee vision/audio seulement si pertinent
        vision = None
        audio = None
        if intent in ["vision_analysis", "multimodal"]:
            vision = await self._get_vision_context()
        if intent in ["audio_processing", "multimodal"]:
            audio = await self._get_audio_context()

        system_state = await self._get_system_state()
        documents = await self._get_documents_context()

        return self._merge_contexts(
            memory=memory,
            rag=rag,
            vision=vision,
            system_state=system_state,
            audio=audio,
            documents=documents
        )

    async def _classify_intent(self, message: str) -> str:
        """Classifier l'intent du message"""
        # Simple keyword-based pour commencer
        message_lower = message.lower()

        if any(kw in message_lower for kw in ["r√®gle", "rule", "policy", "principe"]):
            return "rules_query"
        elif any(kw in message_lower for kw in ["projet", "code", "function", "class"]):
            return "project_query"
        elif any(kw in message_lower for kw in ["rappelle", "remember", "conversation pr√©c√©dente"]):
            return "memory_query"
        elif any(kw in message_lower for kw in ["image", "voir", "analyser", "vision"]):
            return "vision_analysis"
        elif any(kw in message_lower for kw in ["audio", "son", "voix", "√©coute"]):
            return "audio_processing"
        else:
            return "general"

    def _get_rag_config_for_intent(self, intent: str) -> Dict[str, int]:
        """Configuration RAG selon l'intent"""
        configs = {
            "rules_query": {
                "agent_core": 5,
                "rules": 3,
                "projects": 0,
                "scratchpad": 0
            },
            "project_query": {
                "projects": 5,
                "agent_core": 1,
                "scratchpad": 2
            },
            "memory_query": {
                "agent_memory": 5,
                "context_flow": 3,
                "scratchpad": 0
            },
            "general": {
                "agent_core": 2,
                "projects": 2,
                "scratchpad": 1,
                "rules": 1
            }
        }
        return configs.get(intent, configs["general"])

    async def _get_adaptive_rag_context(self, user_message: str, config: Dict[str, int]) -> Dict[str, Any]:
        """Requ√™te RAG adapt√©e selon config"""
        rag_results = {}

        for dataset, top_k in config.items():
            if top_k > 0:
                results = await self.orchestrator.rag_client.query(dataset, user_message, top_k=top_k)
                if results:
                    rag_results[dataset] = results

        return {
            "status": "success",
            "datasets": rag_results,
            "total_results": sum(len(v) for v in rag_results.values())
        }
```

#### C2. Optimiser Embeddings Storage

**Objectif:** Stocker embeddings en binaire, pas en JSON

**Migration:**
```sql
-- Ajouter colonne binaire
ALTER TABLE chunks ADD COLUMN embedding_bin BLOB;

-- Script de migration
UPDATE chunks
SET embedding_bin = (
    -- Convertir JSON ‚Üí liste ‚Üí bytes numpy ‚Üí blob
    -- √Ä faire en Python
);

-- Supprimer ancienne colonne (apr√®s migration)
-- ALTER TABLE chunks DROP COLUMN embedding;
```

**Code:**
```python
import struct

class OptimizedRAGStore(RAGStore):
    def _embedding_to_bytes(self, embedding: List[float]) -> bytes:
        """Convertir embedding en bytes binaires"""
        return struct.pack(f'{len(embedding)}f', *embedding)

    def _bytes_to_embedding(self, data: bytes) -> List[float]:
        """Convertir bytes en embedding"""
        n = len(data) // 4  # 4 bytes par float
        return list(struct.unpack(f'{n}f', data))

    async def add_document(self, dataset, filename, content, metadata):
        # ...
        for idx, chunk in enumerate(chunks):
            embedding = await self._get_embedding(chunk)
            embedding_bytes = self._embedding_to_bytes(embedding)

            cursor.execute("""
                INSERT OR REPLACE INTO chunks
                (id, document_id, chunk, embedding_bin, order_index, created_at)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (chunk_id, doc_id, chunk, embedding_bytes, idx, now))
```

**Gain:** Taille r√©duite de 70-80%, parsing 10x plus rapide.

#### C3. Indexation Vectorielle (FAISS)

**Objectif:** Utiliser FAISS pour recherche vectorielle rapide

**Impl√©mentation:**
```python
import faiss
import numpy as np

class FAISSRAGStore(RAGStore):
    def __init__(self, db_path: str = "rag/rag.db"):
        super().__init__(db_path)
        self.index = None
        self.chunk_ids = []
        self._load_index()

    def _load_index(self):
        """Charger tous les embeddings en index FAISS"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT id, embedding FROM chunks WHERE embedding IS NOT NULL")
        rows = cursor.fetchall()
        conn.close()

        if not rows:
            # Index vide
            self.index = faiss.IndexFlatL2(384)  # 384 dimensions
            return

        # Construire l'index
        chunk_ids = []
        embeddings = []

        for chunk_id, embedding_json in rows:
            chunk_ids.append(chunk_id)
            embeddings.append(json.loads(embedding_json))

        self.chunk_ids = chunk_ids
        embeddings_np = np.array(embeddings, dtype=np.float32)

        # Cr√©er index FAISS
        self.index = faiss.IndexFlatL2(embeddings_np.shape[1])
        self.index.add(embeddings_np)

    async def query(self, dataset: str, question: str, top_k: int = 5):
        """Recherche ultra-rapide avec FAISS"""
        # G√©n√©rer embedding de la question
        question_embedding = await self._get_embedding(question)
        query_vec = np.array([question_embedding], dtype=np.float32)

        # Recherche FAISS
        distances, indices = self.index.search(query_vec, top_k)

        # R√©cup√©rer les chunks correspondants
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        results = []
        for idx, distance in zip(indices[0], distances[0]):
            chunk_id = self.chunk_ids[idx]

            cursor.execute("""
                SELECT c.chunk, d.filename, d.metadata, d.dataset
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE c.id = ?
            """, (chunk_id,))

            row = cursor.fetchone()
            if row and row[3] == dataset:  # Filtrer par dataset
                content, filename, metadata_json, _ = row
                similarity = 1 / (1 + distance)  # Convertir distance ‚Üí similarit√©

                results.append({
                    "chunk_id": chunk_id,
                    "content": content,
                    "filename": filename,
                    "metadata": json.loads(metadata_json) if metadata_json else {},
                    "similarity": float(similarity)
                })

        conn.close()

        # Trier par similarit√©
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]
```

**Gain:** Recherche 100x plus rapide sur gros volumes (>10k chunks).

#### C4. Structurer Memory Hi√©rarchiquement

**Objectif:** Organiser sessions par projet/date

**Structure:**
```
memory_data/
‚îú‚îÄ‚îÄ active/              # Sessions actives (< 7 jours)
‚îÇ   ‚îú‚îÄ‚îÄ default.json
‚îÇ   ‚îî‚îÄ‚îÄ session_xxx.json
‚îú‚îÄ‚îÄ archive/             # Sessions anciennes (7-90 jours)
‚îÇ   ‚îú‚îÄ‚îÄ 2025-11/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_xxx.json
‚îÇ   ‚îî‚îÄ‚îÄ 2025-10/
‚îú‚îÄ‚îÄ projects/            # Sessions par projet
‚îÇ   ‚îú‚îÄ‚îÄ project_alpha/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_xxx.json
‚îÇ   ‚îî‚îÄ‚îÄ project_beta/
‚îî‚îÄ‚îÄ tests/               # Sessions de test
    ‚îî‚îÄ‚îÄ test_xxx.json
```

**Impl√©mentation:**
```python
class HierarchicalMemoryManager(MemoryManager):
    def __init__(self, storage_dir: Optional[str] = None):
        super().__init__(storage_dir)
        self.active_dir = self.storage_dir / "active"
        self.archive_dir = self.storage_dir / "archive"
        self.projects_dir = self.storage_dir / "projects"
        self.tests_dir = self.storage_dir / "tests"

        # Cr√©er sous-r√©pertoires
        for directory in [self.active_dir, self.archive_dir, self.projects_dir, self.tests_dir]:
            directory.mkdir(exist_ok=True)

    def _get_session_file(self, session_id: str, metadata: Dict[str, Any] = None) -> Path:
        """D√©terminer le chemin du fichier selon metadata"""
        safe_session_id = "".join(c for c in session_id if c.isalnum() or c in ('-', '_'))

        # Test sessions
        if session_id.startswith("test_"):
            return self.tests_dir / f"{safe_session_id}.json"

        # Project sessions
        if metadata and "project" in metadata:
            project_dir = self.projects_dir / metadata["project"]
            project_dir.mkdir(exist_ok=True)
            return project_dir / f"{safe_session_id}.json"

        # Active sessions (default)
        return self.active_dir / f"{safe_session_id}.json"

    async def auto_archive_old_sessions(self, days_threshold: int = 7):
        """Archiver automatiquement les sessions anciennes"""
        cutoff_date = datetime.now() - timedelta(days=days_threshold)
        archived_count = 0

        for session_file in self.active_dir.glob("*.json"):
            mtime = datetime.fromtimestamp(session_file.stat().st_mtime)

            if mtime < cutoff_date:
                # Archiver dans archive/{year-month}/
                year_month = mtime.strftime("%Y-%m")
                archive_month_dir = self.archive_dir / year_month
                archive_month_dir.mkdir(exist_ok=True)

                session_file.rename(archive_month_dir / session_file.name)
                archived_count += 1

        return {"archived": archived_count, "threshold_days": days_threshold}
```

---

## PLAN D'ACTION RECOMMAND√â

### Phase 1 : Stabilisation (Semaine 1-2)

**Objectif:** Corriger les probl√®mes critiques

‚úÖ **Actions:**
1. **Migrer les datasets RAG**
   - Script de migration `test_dataset` ‚Üí `projects`, `uploads` ‚Üí `scratchpad`
   - Valider que tous les docs sont dans les bons datasets
   - Ajouter validation stricte √† l'API

2. **Supprimer double syst√®me sessions**
   - Supprimer code `SessionMemory` dans `rag_engine.py`
   - Supprimer r√©pertoire `memory/sessions/`
   - Mettre √† jour documentation

3. **Impl√©menter Timeline persistante**
   - Cr√©er table SQLite `timeline_events`
   - Migrer `Timeline` ‚Üí `PersistentTimeline`
   - Tester sauvegarde/restauration

4. **Documentation architecture**
   - Documenter les 5 datasets et leurs usages
   - Documenter Memory vs RAG
   - Cr√©er sch√©mas d'architecture

üìä **KPI de succ√®s:**
- ‚úÖ 100% des docs dans datasets corrects
- ‚úÖ Z√©ro duplication syst√®me sessions
- ‚úÖ Timeline survit aux red√©marrages
- ‚úÖ Documentation √† jour

### Phase 2 : Enrichissement (Semaine 3-4)

**Objectif:** Am√©liorer fonctionnalit√©s existantes

‚úÖ **Actions:**
1. **Embeddings dans Memory**
   - Ajouter `embedding_model` √† `MemoryManager`
   - G√©n√©rer embeddings pour chaque message
   - Impl√©menter `semantic_search()`

2. **M√©tadonn√©es intelligentes**
   - Impl√©menter auto-routing par type
   - Ajouter filtres metadata aux requ√™tes
   - Documenter metadata schema

3. **Chunking intelligent**
   - Int√©grer `RecursiveCharacterTextSplitter`
   - Tester avec docs longs
   - Comparer qualit√© vs chunking na√Øf

4. **Cleanup automatique**
   - Impl√©menter `cleanup_scratchpad(retention_days=7)`
   - Impl√©menter `cleanup_old_sessions(retention_days=30)`
   - Configurer t√¢che cron quotidienne

üìä **KPI de succ√®s:**
- ‚úÖ Recherche s√©mantique op√©rationnelle dans Memory
- ‚úÖ Auto-routing fonctionne pour 100% des ajouts
- ‚úÖ Qualit√© chunks am√©lior√©e (mesure humaine)
- ‚úÖ Cleanup automatique tourne quotidiennement

### Phase 3 : Optimisation (Semaine 5-6)

**Objectif:** Performance et scalabilit√©

‚úÖ **Actions:**
1. **Embeddings binaires**
   - Migrer embeddings JSON ‚Üí BLOB
   - Mesurer gain taille/performance
   - Valider r√©sultats identiques

2. **Context Builder adaptatif**
   - Impl√©menter classification intent
   - Configurer RAG adaptatif
   - Mesurer r√©duction tokens

3. **Memory hi√©rarchique**
   - Restructurer `memory_data/` en `active/archive/projects/tests/`
   - Migrer sessions existantes
   - Auto-archivage quotidien

4. **Versioning documents (optionnel)**
   - Cr√©er table `document_versions`
   - Impl√©menter historique
   - Interface pour voir versions

üìä **KPI de succ√®s:**
- ‚úÖ Taille DB r√©duite de 70%
- ‚úÖ Parsing embeddings 10x plus rapide
- ‚úÖ Context tokens r√©duits de 30-50% selon intent
- ‚úÖ Sessions organis√©es logiquement

### Phase 4 : Scalabilit√© (Semaine 7-8+)

**Objectif:** Supporter gros volumes

‚úÖ **Actions (optionnelles):**
1. **FAISS indexation**
   - Int√©grer FAISS pour recherche vectorielle
   - Benchmark vs SQLite native
   - Migration progressive

2. **Pagination avanc√©e**
   - Impl√©menter pagination API
   - Cursor-based pagination
   - Limites raisonnables par d√©faut

3. **Monitoring & m√©triques**
   - Ajouter m√©triques Prometheus
   - Dashboard Grafana
   - Alertes sur anomalies

üìä **KPI de succ√®s:**
- ‚úÖ Support >100k chunks sans d√©gradation
- ‚úÖ Recherche <100ms m√™me sur gros volumes
- ‚úÖ M√©triques temps r√©el disponibles

---

## CONCLUSION

### R√©sum√© Ex√©cutif

Le syst√®me RAG & Memory actuel est **fonctionnel mais sous-optimis√©**. Les fondations sont solides (SQLite + embeddings locaux pour RAG, JSON pour Memory), mais plusieurs probl√®mes architecturaux limitent l'efficacit√© :

**Points Forts:**
‚úÖ S√©paration claire RAG (long-terme) vs Memory (court-terme)
‚úÖ Embeddings locaux (pas de d√©pendance API)
‚úÖ Architecture modulaire (clients MCP)
‚úÖ Context Builder pour agr√©gation

**Points Faibles:**
‚ùå Datasets conceptuels non utilis√©s
‚ùå Double syst√®me sessions
‚ùå Memory sans recherche s√©mantique
‚ùå Timeline non persistante
‚ùå M√©tadonn√©es non exploit√©es

### Impact des Am√©liorations

**Court terme (Phase 1):**
- üéØ Clart√© architecturale
- üéØ Tra√ßabilit√© compl√®te (timeline)
- üéØ Organisation logique des donn√©es

**Moyen terme (Phases 2-3):**
- üéØ Recherche s√©mantique partout
- üéØ Auto-organisation intelligente
- üéØ Performance optimis√©e
- üéØ Maintenance automatis√©e

**Long terme (Phase 4+):**
- üéØ Scalabilit√© illimit√©e
- üéØ Observabilit√© compl√®te
- üéØ Production-ready

### Recommandation Finale

**Priorit√©:** Commencer par la **Phase 1** imm√©diatement. Les corrections des probl√®mes critiques sont essentielles pour √©viter :
- Confusion sur l'organisation des donn√©es
- Perte de tra√ßabilit√©
- Duplication code/donn√©es
- Tech debt croissante

**Timeline recommand√©e:** 8 semaines pour compl√©ter Phases 1-3, Phase 4 optionnelle selon besoins.

**Ressources n√©cessaires:**
- 1 d√©veloppeur √† temps plein
- Environnement de test
- Backup complet avant migration

---

## ANNEXES

### A. Sch√©ma Base de Donn√©es Actuel

```sql
-- rag.db (SQLite)
documents (
    id TEXT PRIMARY KEY,
    dataset TEXT NOT NULL,
    filename TEXT NOT NULL,
    content TEXT NOT NULL,
    metadata TEXT,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
)

chunks (
    id TEXT PRIMARY KEY,
    document_id TEXT NOT NULL,
    chunk TEXT NOT NULL,
    embedding TEXT,  -- JSON [384 floats]
    order_index INTEGER NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
)
```

### B. Sch√©ma Propos√© Complet

```sql
-- rag.db (SQLite) - Version am√©lior√©e
documents (
    id TEXT PRIMARY KEY,
    dataset TEXT NOT NULL CHECK(dataset IN ('agent_core', 'context_flow', 'agent_memory', 'projects', 'scratchpad')),
    filename TEXT NOT NULL,
    content TEXT NOT NULL,
    metadata TEXT,  -- JSON
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    created_by TEXT,
    current_version INTEGER DEFAULT 1
)

chunks (
    id TEXT PRIMARY KEY,
    document_id TEXT NOT NULL,
    chunk TEXT NOT NULL,
    embedding_bin BLOB,  -- Binaire optimis√©
    order_index INTEGER NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
)

document_versions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id TEXT NOT NULL,
    version INTEGER NOT NULL,
    content TEXT NOT NULL,
    metadata TEXT,
    created_at TEXT NOT NULL,
    created_by TEXT,
    change_reason TEXT,
    UNIQUE(document_id, version)
)

timeline_events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,
    session_id TEXT NOT NULL,
    event_type TEXT NOT NULL,
    data TEXT NOT NULL,  -- JSON
    metadata TEXT,  -- JSON
    modality TEXT
)

-- Indexes
CREATE INDEX idx_documents_dataset ON documents(dataset);
CREATE INDEX idx_chunks_document ON chunks(document_id);
CREATE INDEX idx_versions_doc ON document_versions(document_id);
CREATE INDEX idx_timeline_session ON timeline_events(session_id);
CREATE INDEX idx_timeline_type ON timeline_events(event_type);
CREATE INDEX idx_timeline_modality ON timeline_events(modality);
```

### C. Structure Memory Hi√©rarchique

```
memory_data/
‚îú‚îÄ‚îÄ active/
‚îÇ   ‚îú‚îÄ‚îÄ default.json
‚îÇ   ‚îî‚îÄ‚îÄ session_*.json
‚îú‚îÄ‚îÄ archive/
‚îÇ   ‚îú‚îÄ‚îÄ 2025-11/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_*.json
‚îÇ   ‚îî‚îÄ‚îÄ 2025-10/
‚îÇ       ‚îî‚îÄ‚îÄ session_*.json
‚îú‚îÄ‚îÄ projects/
‚îÇ   ‚îú‚îÄ‚îÄ agent_local/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_*.json
‚îÇ   ‚îú‚îÄ‚îÄ mcp_documents/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_*.json
‚îÇ   ‚îî‚îÄ‚îÄ rag_audit/
‚îÇ       ‚îî‚îÄ‚îÄ session_*.json
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ integration_test.json
    ‚îî‚îÄ‚îÄ end_to_end_test.json
```

### D. Datasets RAG - Guide d'Utilisation

| Dataset | Usage | R√©tention | Exemples |
|---------|-------|-----------|----------|
| `agent_core` | Identit√©, r√®gles permanentes, structure PC | Permanent | Capacit√©s agent, r√®gles comportement, config syst√®me |
| `context_flow` | R√©sum√©s conversations, flux contexte | 90 jours | R√©sum√© session pr√©c√©dente, contexte projet actif |
| `agent_memory` | Feedbacks, le√ßons, apprentissages | Permanent | "User pr√©f√®re format JSON", "√âviter lib X" |
| `projects` | Code, docs analytiques, travail en cours | 180 jours | Documentation code, architecture, specs |
| `scratchpad` | Donn√©es temporaires, tests | 7 jours | Notes temporaires, r√©sultats tests, brouillons |

### E. M√©triques de Monitoring Recommand√©es

```python
# Prometheus metrics
rag_documents_total = Counter('rag_documents_total', 'Total documents', ['dataset'])
rag_chunks_total = Counter('rag_chunks_total', 'Total chunks', ['dataset'])
rag_query_duration_seconds = Histogram('rag_query_duration_seconds', 'Query duration')
rag_query_results = Histogram('rag_query_results', 'Number of results returned')

memory_sessions_total = Counter('memory_sessions_total', 'Total sessions')
memory_messages_total = Counter('memory_messages_total', 'Total messages', ['role'])
memory_search_duration_seconds = Histogram('memory_search_duration_seconds', 'Search duration')

timeline_events_total = Counter('timeline_events_total', 'Total events', ['event_type', 'modality'])
```

---

**Fin du Rapport d'Audit**

*Rapport g√©n√©r√© le 21 Novembre 2025*
*Version: 1.0*
*Auditeur: Claude Agent*
