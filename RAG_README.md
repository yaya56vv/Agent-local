# ü§ñ Module RAG - Agent Local

## Vue d'ensemble

Module RAG (Retrieval-Augmented Generation) complet avec support pour LLM local (Ollama / LM Studio).

### Fonctionnalit√©s

‚úÖ **Stockage vectoriel** - Embeddings via Gemini API + SQLite
‚úÖ **Recherche s√©mantique** - Similarit√© cosinus sur les embeddings
‚úÖ **LLM local** - Support Ollama et LM Studio
‚úÖ **API REST compl√®te** - Endpoints FastAPI
‚úÖ **Interface web** - UI moderne et responsive
‚úÖ **Multi-datasets** - Gestion de collections s√©par√©es

---

## üöÄ Installation

### 1. D√©pendances Python

```bash
pip install -r requirements.txt
```

### 2. LLM Local (choisir un)

#### Option A : Ollama (recommand√©)
```bash
# Installer Ollama
# T√©l√©charger depuis: https://ollama.ai

# T√©l√©charger un mod√®le
ollama pull llama3.2
# ou
ollama pull qwen2.5:14b
```

#### Option B : LM Studio
```bash
# T√©l√©charger LM Studio
# https://lmstudio.ai

# Lancer un mod√®le dans LM Studio
# Default port: 1234
```

### 3. Variables d'environnement

Cr√©er un fichier `.env` :

```env
# Gemini API pour embeddings
GEMINI_API_KEY=votre_cl√©_api

# LLM Local (Ollama)
LOCAL_LLM_BASE_URL=http://127.0.0.1:11434
LOCAL_LLM_MODEL=llama3.2

# Ou LM Studio
# LOCAL_LLM_BASE_URL=http://127.0.0.1:1234
# LOCAL_LLM_MODEL=local-model
```

---

## üìñ Utilisation

### D√©marrer le serveur

```bash
# Via script PowerShell
.\run_agent.ps1

# Ou directement
cd backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Interface Web

Ouvrir dans le navigateur :
```
http://localhost:8000/ui/rag.html
```

### API REST

#### Ajouter un document
```bash
POST http://localhost:8000/rag/documents/add
Content-Type: application/json

{
  "dataset": "mon_projet",
  "filename": "doc.txt",
  "content": "Contenu du document...",
  "metadata": {"source": "manual"}
}
```

#### Poser une question
```bash
POST http://localhost:8000/rag/query
Content-Type: application/json

{
  "dataset": "mon_projet",
  "question": "Quelle est la fonction principale ?",
  "top_k": 5,
  "use_llm": true
}
```

#### Lister les datasets
```bash
GET http://localhost:8000/rag/datasets
```

#### Supprimer un dataset
```bash
DELETE http://localhost:8000/rag/datasets/mon_projet
```

#### V√©rifier le statut LLM
```bash
GET http://localhost:8000/rag/llm/status
```

---

## üîß Int√©gration avec l'orchestrator

### Utilisation simple

```python
from backend.rag.rag_helper import answer_question_with_rag

# R√©pondre √† une question
answer = await answer_question_with_rag(
    dataset="mon_projet",
    question="Comment fonctionne le syst√®me ?"
)
print(answer)
```

### Utilisation avanc√©e

```python
from backend.rag.rag_helper import rag_helper

# R√©ponse compl√®te avec sources
result = await rag_helper.answer_with_rag(
    dataset="mon_projet",
    question="Comment fonctionne le syst√®me ?",
    top_k=5,
    temperature=0.7
)

print(result["answer"])
print(f"Sources: {len(result['sources'])}")
for source in result["sources"]:
    print(f"- {source['filename']}: {source['similarity']:.2%}")
```

### Ajouter des documents

```python
from backend.rag.rag_helper import rag_helper

# Ajouter un document
doc_id = rag_helper.add_document_sync(
    dataset="mon_projet",
    filename="readme.md",
    content="# Mon Projet\n\nDescription...",
    metadata={"type": "documentation"}
)
```

---

## üìÅ Structure des fichiers

```
backend/
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ rag_store.py       # Stockage vectoriel SQLite + Gemini
‚îÇ   ‚îú‚îÄ‚îÄ rag_helper.py      # Helper pour orchestrator
‚îÇ   ‚îî‚îÄ‚îÄ rag_engine.py      # (existant)
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îî‚îÄ‚îÄ rag_routes.py      # API REST endpoints
‚îú‚îÄ‚îÄ connectors/
‚îÇ   ‚îî‚îÄ‚îÄ local_llm/
‚îÇ       ‚îú‚îÄ‚îÄ local_llm_connector.py  # Connecteur Ollama/LM Studio
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ main.py                # Application FastAPI

frontend/
‚îî‚îÄ‚îÄ ui/
    ‚îú‚îÄ‚îÄ rag.html           # Interface web
    ‚îî‚îÄ‚îÄ rag.js             # Logique frontend

rag/                       # Donn√©es RAG
‚îú‚îÄ‚îÄ rag.db                 # Base SQLite
‚îî‚îÄ‚îÄ documents/             # (optionnel)
```

---

## üß™ Tests

### Test rapide

```python
import asyncio
from backend.rag.rag_helper import rag_helper

async def test_rag():
    # Ajouter un document
    doc_id = rag_helper.add_document_sync(
        dataset="test",
        filename="test.txt",
        content="Python est un langage de programmation interpr√©t√©."
    )
    print(f"Document ajout√©: {doc_id}")
    
    # V√©rifier LLM
    llm_ok = await rag_helper.check_llm_available()
    print(f"LLM disponible: {llm_ok}")
    
    # Poser une question
    result = await rag_helper.answer_with_rag(
        dataset="test",
        question="Qu'est-ce que Python ?"
    )
    print(f"R√©ponse: {result['answer']}")

asyncio.run(test_rag())
```

---

## üîç Endpoints API complets

| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/rag/documents/add` | Ajouter un document |
| POST | `/rag/documents/upload` | Upload fichier |
| POST | `/rag/query` | Recherche + g√©n√©ration LLM |
| GET | `/rag/datasets` | Liste des datasets |
| GET | `/rag/datasets/{dataset}` | Info d'un dataset |
| DELETE | `/rag/datasets/{dataset}` | Supprimer dataset |
| GET | `/rag/llm/status` | Statut du LLM |
| POST | `/rag/llm/configure` | Configurer LLM |
| GET | `/rag/health` | Health check |

---

## ‚öôÔ∏è Configuration avanc√©e

### Changer de mod√®le Ollama

```bash
# Lister les mod√®les disponibles
ollama list

# T√©l√©charger un nouveau mod√®le
ollama pull qwen2.5:14b

# Mettre √† jour .env
LOCAL_LLM_MODEL=qwen2.5:14b
```

### Utiliser LM Studio

1. Lancer LM Studio
2. Charger un mod√®le
3. D√©marrer le serveur local
4. Configurer dans `.env` :

```env
LOCAL_LLM_BASE_URL=http://127.0.0.1:1234
LOCAL_LLM_MODEL=local-model
```

### Personnaliser les embeddings

Modifier dans `backend/rag/rag_store.py` :

```python
self.embedding_model = "models/text-embedding-004"  # Mod√®le Gemini
```

### Ajuster le chunking

```python
chunks = self._chunk_text(
    content, 
    chunk_size=1000,    # Taille des chunks
    overlap=200         # Overlap entre chunks
)
```

---

## üêõ D√©pannage

### LLM non disponible

```bash
# V√©rifier Ollama
ollama list
curl http://localhost:11434/api/tags

# V√©rifier LM Studio
curl http://localhost:1234/v1/models
```

### Erreur d'embeddings

- V√©rifier `GEMINI_API_KEY` dans `.env`
- Tester la cl√© : https://ai.google.dev/

### Base de donn√©es corrompue

```bash
# Supprimer et recr√©er
rm rag/rag.db
# La base sera recr√©√©e au prochain lancement
```

### CORS errors

V√©rifier que le backend autorise CORS :
```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## üìö Exemples d'utilisation

### Exemple 1 : Documentation de projet

```python
# Ajouter toute la doc
docs = [
    ("readme.md", readme_content),
    ("api.md", api_content),
    ("guide.md", guide_content)
]

for filename, content in docs:
    rag_helper.add_document_sync(
        dataset="project_docs",
        filename=filename,
        content=content
    )

# Interroger
answer = await answer_question_with_rag(
    dataset="project_docs",
    question="Comment utiliser l'API ?"
)
```

### Exemple 2 : Base de connaissances

```python
# Cr√©er plusieurs datasets th√©matiques
datasets = {
    "python": ["python_basics.txt", "python_advanced.txt"],
    "javascript": ["js_guide.txt", "react_intro.txt"],
    "devops": ["docker.txt", "kubernetes.txt"]
}

# Poser une question cibl√©e
answer = await answer_question_with_rag(
    dataset="python",
    question="Comment cr√©er un d√©corateur ?"
)
```

---

## üéØ Prochaines √©tapes

1. **Lancer Ollama** et t√©l√©charger un mod√®le
2. **Configurer** les variables d'environnement
3. **Tester** l'interface web : http://localhost:8000/ui/rag.html
4. **Ajouter** vos premiers documents
5. **Int√©grer** avec l'orchestrator

---

## üìù Notes

- **Embeddings** : Utilise Gemini API (gratuit avec quotas)
- **Stockage** : SQLite (pas besoin de serveur externe)
- **LLM** : 100% local (pas de co√ªts API)
- **Performance** : D√©pend du mod√®le LLM choisi
- **S√©curit√©** : Donn√©es stock√©es localement

---

## ü§ù Support

Pour toute question ou probl√®me :
1. V√©rifier les logs du backend
2. Tester les endpoints avec curl/Postman
3. V√©rifier que Ollama/LM Studio est lanc√©
4. Consulter la documentation Ollama : https://ollama.ai/docs
