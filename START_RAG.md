# ğŸš€ SYSTÃˆME RAG - READY TO USE

## âœ… ImplÃ©mentation complÃ¨te terminÃ©e !

Le systÃ¨me RAG (Retrieval-Augmented Generation) est maintenant entiÃ¨rement fonctionnel avec :
- âœ… Stockage vectoriel SQLite + Gemini embeddings
- âœ… LLM local (Ollama / LM Studio)
- âœ… API REST FastAPI complÃ¨te
- âœ… Interface web moderne
- âœ… Scripts utilitaires
- âœ… Documentation complÃ¨te

---

## ğŸ¯ DÃ‰MARRAGE IMMÃ‰DIAT (3 Ã©tapes)

### Ã‰tape 1 : Installer Ollama (2 minutes)

**Windows :**
1. TÃ©lÃ©charger : https://ollama.ai/download
2. Installer et lancer
3. Ouvrir PowerShell :
```powershell
ollama pull llama3.2
```

**VÃ©rifier :**
```powershell
ollama list
```

### Ã‰tape 2 : Configurer l'API Gemini (1 minute)

1. Obtenir clÃ© gratuite : https://ai.google.dev/
2. CrÃ©er fichier `.env` Ã  la racine du projet :
```env
GEMINI_API_KEY=votre_clÃ©_ici
LOCAL_LLM_BASE_URL=http://127.0.0.1:11434
LOCAL_LLM_MODEL=llama3.2
```

### Ã‰tape 3 : Tester (30 secondes)

```powershell
python test_rag.py
```

âœ… Si tout est OK, vous verrez :
- LLM local disponible âœ…
- Document ajoutÃ© âœ…
- Recherche fonctionnelle âœ…
- RÃ©ponse gÃ©nÃ©rÃ©e âœ…

---

## ğŸŒ UTILISATION

### Interface Web (le plus simple)

1. **Lancer le serveur :**
```powershell
.\run_agent.ps1
```

2. **Ouvrir dans le navigateur :**
```
http://localhost:8000/ui/rag.html
```

3. **Utiliser :**
   - Entrer un nom de dataset
   - Coller du texte
   - Cliquer "Ajouter au RAG"
   - Poser une question
   - Voir la rÃ©ponse + sources

### Scripts (pour automatiser)

**Ajouter un fichier :**
```powershell
python add_to_rag.py --file README.md --dataset docs
```

**Ajouter un rÃ©pertoire complet :**
```powershell
python add_to_rag.py --dir ./backend --dataset code
```

**Exemples avancÃ©s :**
```powershell
python examples_rag.py
```

### Python (pour intÃ©grer)

```python
import asyncio
from backend.rag.rag_helper import answer_question_with_rag

async def main():
    answer = await answer_question_with_rag(
        dataset="docs",
        question="Comment Ã§a marche ?"
    )
    print(answer)

asyncio.run(main())
```

---

## ğŸ“š DOCUMENTATION

| Fichier | Description |
|---------|-------------|
| **QUICKSTART_RAG.md** | Guide de dÃ©marrage rapide (5 min) |
| **RAG_README.md** | Documentation complÃ¨te (tout) |
| **RAG_IMPLEMENTATION.md** | DÃ©tails de l'implÃ©mentation |
| Ce fichier | Instructions immÃ©diates |

---

## ğŸ”§ ENDPOINTS API

Base URL : `http://localhost:8000`

| Endpoint | MÃ©thode | Description |
|----------|---------|-------------|
| `/rag/documents/add` | POST | Ajouter un document |
| `/rag/query` | POST | Rechercher + gÃ©nÃ©rer rÃ©ponse |
| `/rag/datasets` | GET | Lister les datasets |
| `/rag/datasets/{name}` | GET | Info d'un dataset |
| `/rag/datasets/{name}` | DELETE | Supprimer un dataset |
| `/rag/llm/status` | GET | Status du LLM local |
| `/ui/rag.html` | GET | Interface web |
| `/docs` | GET | Documentation API |

---

## ğŸ“ EXEMPLES RAPIDES

### CrÃ©er une base de docs projet
```powershell
# Indexer le code
python add_to_rag.py --dir ./backend --dataset backend_code --ext .py

# Indexer la doc
python add_to_rag.py --dir . --dataset project_docs --ext .md .txt

# Poser une question via l'interface web
```

### API cURL
```bash
# Ajouter document
curl -X POST http://localhost:8000/rag/documents/add \
  -H "Content-Type: application/json" \
  -d '{
    "dataset": "test",
    "filename": "doc.txt",
    "content": "Python est un langage..."
  }'

# Poser question
curl -X POST http://localhost:8000/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "dataset": "test",
    "question": "Qu est-ce que Python ?",
    "use_llm": true
  }'
```

---

## ğŸ› DÃ‰PANNAGE RAPIDE

### âŒ "LLM local non disponible"
```powershell
# VÃ©rifier Ollama
ollama list
ollama serve  # Si pas lancÃ©

# Tester
curl http://localhost:11434/api/tags
```

### âŒ "Erreur embedding Gemini"
- VÃ©rifier `GEMINI_API_KEY` dans `.env`
- Tester la clÃ© : https://ai.google.dev/

### âŒ "Module not found"
```powershell
pip install -r requirements.txt
```

### âŒ "CORS error"
- VÃ©rifier que le backend tourne sur port 8000
- Utiliser : http://localhost:8000/ui/rag.html (pas file://)

---

## ğŸ“Š ARCHITECTURE SIMPLIFIÃ‰E

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Interface Web   â”‚  â† Vous utilisez Ã§a
â”‚   rag.html       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Routes     â”‚  â† FastAPI REST API
â”‚  rag_routes.py   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚         â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG     â”‚  â”‚ Local LLM â”‚
â”‚ Store   â”‚  â”‚ Connector â”‚
â”‚ (SQLite)â”‚  â”‚ (Ollama)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… CHECKLIST DE VÃ‰RIFICATION

Avant de commencer, vÃ©rifiez :

- [ ] Python 3.11+ installÃ©
- [ ] Ollama installÃ© et lancÃ© (`ollama list`)
- [ ] ModÃ¨le tÃ©lÃ©chargÃ© (`llama3.2` ou autre)
- [ ] ClÃ© Gemini API obtenue
- [ ] Fichier `.env` crÃ©Ã© avec `GEMINI_API_KEY`
- [ ] DÃ©pendances installÃ©es (`pip install -r requirements.txt`)
- [ ] Test rÃ©ussi (`python test_rag.py`)

Si tout est âœ…, vous Ãªtes prÃªt !

---

## ğŸ¯ PROCHAINES Ã‰TAPES

### Maintenant
1. âœ… Lancer le test : `python test_rag.py`
2. âœ… DÃ©marrer le serveur : `.\run_agent.ps1`
3. âœ… Ouvrir l'interface : http://localhost:8000/ui/rag.html

### Ensuite
4. Ajouter vos premiers documents
5. Tester diffÃ©rentes questions
6. CrÃ©er plusieurs datasets thÃ©matiques

### Plus tard
7. IntÃ©grer dans l'orchestrator
8. CrÃ©er des agents spÃ©cialisÃ©s
9. Optimiser pour vos cas d'usage

---

## ğŸ’¡ CONSEILS

### ModÃ¨les LLM recommandÃ©s

**Pour dÃ©marrer (rapide) :**
```powershell
ollama pull llama3.2  # 2GB, trÃ¨s rapide
```

**Pour la qualitÃ© :**
```powershell
ollama pull qwen2.5:14b  # 8GB, meilleure qualitÃ©
ollama pull mistral:7b   # 4GB, bon compromis
```

### Changer de modÃ¨le
Dans `.env` :
```env
LOCAL_LLM_MODEL=qwen2.5:14b
```
Puis redÃ©marrer le serveur.

### Performances
- Plus de chunks (`top_k`) = plus de contexte mais plus lent
- Temperature Ã©levÃ©e = rÃ©ponses crÃ©atives
- Temperature basse = rÃ©ponses prÃ©cises

---

## ğŸ“ SUPPORT

### Si Ã§a ne marche pas :

1. **VÃ©rifier les logs :**
   - Console du serveur
   - Console du navigateur (F12)

2. **Tester les composants :**
   ```powershell
   # Test Ollama
   curl http://localhost:11434/api/tags
   
   # Test backend
   curl http://localhost:8000/health
   
   # Test RAG
   python test_rag.py
   ```

3. **Documentation :**
   - `QUICKSTART_RAG.md` pour dÃ©marrage
   - `RAG_README.md` pour tout le reste
   - `/docs` endpoint pour API reference

---

## ğŸ‰ PRÃŠT !

Vous avez maintenant un systÃ¨me RAG complet et fonctionnel.

**Pour commencer immÃ©diatement :**

```powershell
# 1. Test rapide
python test_rag.py

# 2. Lancer le serveur
.\run_agent.ps1

# 3. Ouvrir le navigateur
start http://localhost:8000/ui/rag.html
```

**Bon dÃ©veloppement ! ğŸš€**
